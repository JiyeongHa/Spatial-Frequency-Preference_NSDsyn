{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ebe421a5-00e7-4059-b757-87a14d69a2aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "import sfp_nsd_utils as utils\n",
    "import pandas as pd\n",
    "import two_dimensional_model as model\n",
    "import simulation as sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b29b6512-ca4e-4869-8f03-c4580d4b4457",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "used existing dir\n"
     ]
    }
   ],
   "source": [
    "params = pd.DataFrame({'sigma': [2.2], 'slope': [0.12], 'intercept': [0.35],\n",
    "                       'p_1': [0.06], 'p_2': [-0.03], 'p_3': [0.07], 'p_4': [0.005],\n",
    "                       'A_1': [0.04], 'A_2': [-0.01], 'A_3': [0], 'A_4': [0]})\n",
    "\n",
    "subj_data = sim.SynthesizeRealData(sn=1, pw=True, subj_df_dir='/Volumes/server/Projects/sfp_nsd/natural-scenes-dataset/derivatives/dataframes')\n",
    "subj_syn_df_2d = subj_data.synthesize_BOLD_2d(params, full_ver=True)\n",
    "noisy_syn_df_2d = sim.copy_df_and_add_noise(subj_syn_df_2d, beta_col=\"normed_betas\", noise_mean=0, noise_sd=subj_syn_df_2d['noise_SD']*1)\n",
    "output_losses_history = '/Users/jh7685/Documents/Projects/test_3.csv'\n",
    "output_model_history = '/Users/jh7685/Documents/Projects/test_4.csv'\n",
    "output_loss_history = '/Users/jh7685/Documents/Projects/test_5.csv'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "57186da2-c545-42d0-832a-24d741265f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "syn_dataset = model.SpatialFrequencyDataset(noisy_syn_df_2d, beta_col='normed_betas')\n",
    "syn_model = model.SpatialFrequencyModel(syn_dataset.my_tensor, full_ver=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8318b8c1-38cc-4711-95bd-f9ad1538f820",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext line_profiler\n",
    "%load_ext memory_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "97f54b48-d699-426d-b8fa-6a23055f4e03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**epoch no.0 loss: 79.749\n",
      "**epoch no.1: Finished! final model params...\n",
      "{'sigma': 0.08361691236495972, 'slope': 0.9147575497627258, 'intercept': 0.5299279093742371, 'p_1': 0.06939395517110825, 'p_2': 0.08834829926490784, 'p_3': 0.00639725849032402, 'p_4': 0.04498710483312607, 'A_1': 0.2541731297969818, 'A_2': 0.7224149107933044}\n",
      "Elapsed time: 1631.65 sec\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Timer unit: 1e-06 s\n",
       "\n",
       "Total time: 1631.75 s\n",
       "File: ../two_dimensional_model.py\n",
       "Function: fit_model at line 363\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "   363                                           def fit_model(model, dataset, learning_rate=1e-4, max_epoch=1000, print_every=100,\n",
       "   364                                                         loss_all_voxels=True, anomaly_detection=True, amsgrad=False, eps=1e-8):\n",
       "   365                                               \"\"\"Fit the model. This function will allow you to run a for loop for N times set as max_epoch,\n",
       "   366                                               and return the output of the training; loss history, model history.\"\"\"\n",
       "   367         1         14.0     14.0      0.0      torch.autograd.set_detect_anomaly(anomaly_detection)\n",
       "   368                                               # [sigma, slope, intercept, p_1, p_2, p_3, p_4, A_1, A_2]\n",
       "   369         1         48.0     48.0      0.0      my_parameters = [p for p in model.parameters() if p.requires_grad]\n",
       "   370                                           \n",
       "   371         1         70.0     70.0      0.0      optimizer = torch.optim.Adam(my_parameters, lr=learning_rate, amsgrad=amsgrad, eps=eps)\n",
       "   372         1          0.0      0.0      0.0      losses_history = []\n",
       "   373         1          1.0      1.0      0.0      loss_history = []\n",
       "   374         1          0.0      0.0      0.0      model_history = []\n",
       "   375         1          2.0      2.0      0.0      start = timer()\n",
       "   376                                           \n",
       "   377         2          3.0      1.5      0.0      for t in range(max_epoch):\n",
       "   378                                           \n",
       "   379         1      10853.0  10853.0      0.0          pred = model.forward()  # predictions should be put in here\n",
       "   380         1 1621059595.0 1621059595.0     99.3          losses = loss_fn(dataset.voxel_info, dataset.sigma_v_squared, prediction=pred, target=dataset.target)  # loss should be returned here\n",
       "   381         1         90.0     90.0      0.0          loss = torch.mean(losses)\n",
       "   382         1          1.0      1.0      0.0          if loss_all_voxels is True:\n",
       "   383         1         14.0     14.0      0.0              losses_history.append(losses.detach().numpy())\n",
       "   384         1         67.0     67.0      0.0          model_values = [p.detach().numpy().item() for p in model.parameters() if p.requires_grad]  # output needs to be put in there\n",
       "   385         1         14.0     14.0      0.0          loss_history.append(loss.item())\n",
       "   386         1          1.0      1.0      0.0          model_history.append(model_values)  # more than one item here\n",
       "   387         1          1.0      1.0      0.0          if (t + 1) % print_every == 0 or t == 0:\n",
       "   388         1        163.0    163.0      0.0              print(f'**epoch no.{t} loss: {np.round(loss.item(), 3)}')\n",
       "   389                                           \n",
       "   390         1        173.0    173.0      0.0          optimizer.zero_grad()  # clear previous gradients\n",
       "   391         1   10655187.0 10655187.0      0.7          loss.backward()  # compute gradients of all variables wrt loss\n",
       "   392         1        569.0    569.0      0.0          optimizer.step()  # perform updates using calculated gradients\n",
       "   393         1         22.0     22.0      0.0          model.eval()\n",
       "   394         1          2.0      2.0      0.0      end = timer()\n",
       "   395         1          1.0      1.0      0.0      elapsed_time = end - start\n",
       "   396         1         35.0     35.0      0.0      params_col = [name for name, param in model.named_parameters() if param.requires_grad]\n",
       "   397         1        174.0    174.0      0.0      print(f'**epoch no.{max_epoch}: Finished! final model params...\\n{dict(zip(params_col, model_values))}')\n",
       "   398         1         74.0     74.0      0.0      print(f'Elapsed time: {np.round(end - start, 2)} sec')\n",
       "   399         1       7821.0   7821.0      0.0      voxel_list = dataset.voxel_info.unique().numpy().astype(int).tolist()\n",
       "   400                                           \n",
       "   401         1       9612.0   9612.0      0.0      losses_history = pd.DataFrame(np.asarray(losses_history), columns=voxel_list).reset_index().rename(columns={'index': 'epoch'})\n",
       "   402         1       1379.0   1379.0      0.0      loss_history = pd.DataFrame(loss_history, columns=['loss']).reset_index().rename(columns={'index': 'epoch'})\n",
       "   403         1       1653.0   1653.0      0.0      model_history = pd.DataFrame(model_history, columns=params_col).reset_index().rename(columns={'index': 'epoch'})\n",
       "   404                                           \n",
       "   405         1          1.0      1.0      0.0      return loss_history, model_history, elapsed_time, losses_history"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%lprun -f  model.fit_model(syn_model, syn_dataset, learning_rate=0.01, max_epoch=1, print_every=2000, anomaly_detection=False, amsgrad=False, eps=1e-8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0755cb2b-1afd-4758-9e79-f14bb868845b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**epoch no.0 loss: 76.459\n",
      "**epoch no.1: Finished! final model params...\n",
      "{'sigma': 0.10361690819263458, 'slope': 0.8947575688362122, 'intercept': 0.5099279284477234, 'p_1': 0.04939395189285278, 'p_2': 0.06834830343723297, 'p_3': -0.01360274013131857, 'p_4': 0.024987107142806053, 'A_1': 0.2741731107234955, 'A_2': 0.7024149298667908}\n",
      "Elapsed time: 1751.62 sec\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Timer unit: 1e-06 s\n",
       "\n",
       "Total time: 1741.39 s\n",
       "File: ../two_dimensional_model.py\n",
       "Function: loss_fn at line 348\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "   348                                           def loss_fn(voxel_info, sigma_v_info, prediction, target):\n",
       "   349                                               \"\"\"\"\"\"\n",
       "   350         1    1374542.0 1374542.0      0.1      norm_pred = normalize(voxel_info=voxel_info, to_norm=prediction)\n",
       "   351         1    1245504.0 1245504.0      0.1      norm_measured = normalize(voxel_info=voxel_info, to_norm=target)\n",
       "   352         1       1577.0   1577.0      0.0      voxel_list = voxel_info.unique()\n",
       "   353         1         12.0     12.0      0.0      loss_all_voxels = torch.empty(voxel_list.shape, dtype=torch.float64)\n",
       "   354      3952      11023.0      2.8      0.0      for i, idx in zip(range(voxel_list.shape[0]), voxel_list):\n",
       "   355      3951     408974.0    103.5      0.0          voxel_idx = voxel_info == idx\n",
       "   356      3951 1736018695.0 439387.2     99.7          n = 28\n",
       "   357      3951     771216.0    195.2      0.0          sigma_v_squared = sigma_v_info[voxel_idx]\n",
       "   358      3951    1501433.0    380.0      0.1          loss_v = (1/n) * torch.dot((1/sigma_v_squared), ((norm_pred[voxel_idx] - norm_measured[voxel_idx]) ** 2))\n",
       "   359      3951      59482.0     15.1      0.0          loss_all_voxels[i] = loss_v\n",
       "   360         1          1.0      1.0      0.0      return loss_all_voxels"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%lprun -f  model.loss_fn model.fit_model(syn_model, syn_dataset, learning_rate=0.01, max_epoch=1, print_every=2000, anomaly_detection=False, amsgrad=False, eps=1e-8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9959ed99-d876-44f2-99ff-3ea6f72a9087",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**epoch no.0 loss: 73.833\n",
      "**epoch no.1: Finished! final model params...\n",
      "{'sigma': 0.12361690402030945, 'slope': 0.8747575879096985, 'intercept': 0.4899279475212097, 'p_1': 0.02939395233988762, 'p_2': 0.06834830343723297, 'p_3': -0.01360273826867342, 'p_4': 0.004987107589840889, 'A_1': 0.29417309165000916, 'A_2': 0.6824149489402771}\n",
      "Elapsed time: 13.79 sec\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Timer unit: 1e-06 s\n",
       "\n",
       "Total time: 3.59898 s\n",
       "File: ../two_dimensional_model.py\n",
       "Function: loss_fn at line 348\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "   348                                           def loss_fn(voxel_info, sigma_v_info, prediction, target):\n",
       "   349                                               \"\"\"\"\"\"\n",
       "   350         1    1169434.0 1169434.0     32.5      norm_pred = normalize(voxel_info=voxel_info, to_norm=prediction)\n",
       "   351         1    1100912.0 1100912.0     30.6      norm_measured = normalize(voxel_info=voxel_info, to_norm=target)\n",
       "   352         1       1440.0   1440.0      0.0      voxel_list = voxel_info.unique()\n",
       "   353         1         11.0     11.0      0.0      loss_all_voxels = torch.empty(voxel_list.shape, dtype=torch.float64)\n",
       "   354      3952       5936.0      1.5      0.2      for i, idx in zip(range(voxel_list.shape[0]), voxel_list):\n",
       "   355      3951     195383.0     49.5      5.4          voxel_idx = voxel_info == idx\n",
       "   356                                                   #n = sum(voxel_idx)\n",
       "   357      3951     324069.0     82.0      9.0          sigma_v_squared = sigma_v_info[voxel_idx]\n",
       "   358      3951     768412.0    194.5     21.4          loss_v = (1/28) * torch.dot((1/sigma_v_squared), ((norm_pred[voxel_idx] - norm_measured[voxel_idx]) ** 2))\n",
       "   359      3951      33387.0      8.5      0.9          loss_all_voxels[i] = loss_v\n",
       "   360         1          0.0      0.0      0.0      return loss_all_voxels"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%lprun -f  model.loss_fn model.fit_model(syn_model, syn_dataset, learning_rate=0.01, max_epoch=1, print_every=2000, anomaly_detection=False, amsgrad=False, eps=1e-8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f7add8d0-fe49-4190-8ab6-2d9d4d9dccf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**epoch no.0 loss: 78.135\n",
      "**epoch no.1: Finished! final model params...\n",
      "{'sigma': 0.09361691027879715, 'slope': 0.904757559299469, 'intercept': 0.5199279189109802, 'p_1': 0.059393953531980515, 'p_2': 0.0783483013510704, 'p_3': -0.0036027412861585617, 'p_4': 0.034987106919288635, 'A_1': 0.26417312026023865, 'A_2': 0.7124149203300476}\n",
      "Elapsed time: 1638.24 sec\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Filename: ../two_dimensional_model.py\n",
       "\n",
       "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
       "=============================================================\n",
       "   363   3308.8 MiB   3308.8 MiB           1   def fit_model(model, dataset, learning_rate=1e-4, max_epoch=1000, print_every=100,\n",
       "   364                                                       loss_all_voxels=True, anomaly_detection=True, amsgrad=False, eps=1e-8):\n",
       "   365                                             \"\"\"Fit the model. This function will allow you to run a for loop for N times set as max_epoch,\n",
       "   366                                             and return the output of the training; loss history, model history.\"\"\"\n",
       "   367   3308.8 MiB      0.0 MiB           1       torch.autograd.set_detect_anomaly(anomaly_detection)\n",
       "   368                                             # [sigma, slope, intercept, p_1, p_2, p_3, p_4, A_1, A_2]\n",
       "   369   3308.8 MiB      0.0 MiB          12       my_parameters = [p for p in model.parameters() if p.requires_grad]\n",
       "   370                                         \n",
       "   371   3308.8 MiB      0.0 MiB           1       optimizer = torch.optim.Adam(my_parameters, lr=learning_rate, amsgrad=amsgrad, eps=eps)\n",
       "   372   3308.8 MiB      0.0 MiB           1       losses_history = []\n",
       "   373   3308.8 MiB      0.0 MiB           1       loss_history = []\n",
       "   374   3308.8 MiB      0.0 MiB           1       model_history = []\n",
       "   375   3308.8 MiB      0.0 MiB           1       start = timer()\n",
       "   376                                         \n",
       "   377   3403.2 MiB      0.0 MiB           2       for t in range(max_epoch):\n",
       "   378                                         \n",
       "   379   3308.9 MiB      0.1 MiB           1           pred = model.forward()  # predictions should be put in here\n",
       "   380   3778.4 MiB    469.5 MiB           1           losses = loss_fn(dataset.voxel_info, dataset.sigma_v_squared, prediction=pred, target=dataset.target)  # loss should be returned here\n",
       "   381   3778.4 MiB      0.0 MiB           1           loss = torch.mean(losses)\n",
       "   382   3778.4 MiB      0.0 MiB           1           if loss_all_voxels is True:\n",
       "   383   3778.4 MiB      0.0 MiB           1               losses_history.append(losses.detach().numpy())\n",
       "   384   3778.4 MiB      0.0 MiB          12           model_values = [p.detach().numpy().item() for p in model.parameters() if p.requires_grad]  # output needs to be put in there\n",
       "   385   3778.4 MiB      0.0 MiB           1           loss_history.append(loss.item())\n",
       "   386   3778.4 MiB      0.0 MiB           1           model_history.append(model_values)  # more than one item here\n",
       "   387   3778.4 MiB      0.0 MiB           1           if (t + 1) % print_every == 0 or t == 0:\n",
       "   388   3778.4 MiB      0.0 MiB           1               print(f'**epoch no.{t} loss: {np.round(loss.item(), 3)}')\n",
       "   389                                         \n",
       "   390   3778.4 MiB      0.0 MiB           1           optimizer.zero_grad()  # clear previous gradients\n",
       "   391   3403.2 MiB   -375.2 MiB           1           loss.backward()  # compute gradients of all variables wrt loss\n",
       "   392   3403.2 MiB      0.0 MiB           1           optimizer.step()  # perform updates using calculated gradients\n",
       "   393   3403.2 MiB      0.0 MiB           1           model.eval()\n",
       "   394   3403.2 MiB      0.0 MiB           1       end = timer()\n",
       "   395   3403.2 MiB      0.0 MiB           1       elapsed_time = end - start\n",
       "   396   3403.2 MiB      0.0 MiB          12       params_col = [name for name, param in model.named_parameters() if param.requires_grad]\n",
       "   397   3403.2 MiB      0.0 MiB           1       print(f'**epoch no.{max_epoch}: Finished! final model params...\\n{dict(zip(params_col, model_values))}')\n",
       "   398   3403.2 MiB      0.0 MiB           1       print(f'Elapsed time: {np.round(end - start, 2)} sec')\n",
       "   399   3403.2 MiB      0.0 MiB           1       voxel_list = dataset.voxel_info.unique().numpy().astype(int).tolist()\n",
       "   400                                         \n",
       "   401   3403.4 MiB      0.2 MiB           1       losses_history = pd.DataFrame(np.asarray(losses_history), columns=voxel_list).reset_index().rename(columns={'index': 'epoch'})\n",
       "   402   3403.4 MiB      0.0 MiB           1       loss_history = pd.DataFrame(loss_history, columns=['loss']).reset_index().rename(columns={'index': 'epoch'})\n",
       "   403   3403.4 MiB      0.0 MiB           1       model_history = pd.DataFrame(model_history, columns=params_col).reset_index().rename(columns={'index': 'epoch'})\n",
       "   404                                         \n",
       "   405   3403.4 MiB      0.0 MiB           1       return loss_history, model_history, elapsed_time, losses_history"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%mprun -f model.fit_model model.fit_model(syn_model, syn_dataset, learning_rate=0.01, max_epoch=1, print_every=2000, anomaly_detection=False, amsgrad=False, eps=1e-8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0246b005-5f75-458c-b8bf-472080c3fa55",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses_history = model.shape_losses_history(losses, syn_df)\n",
    "utils.save_df_to_csv(losses_history, output_losses_history, indexing=False)\n",
    "utils.save_df_to_csv(syn_model_history, output_model_history, indexing=False)\n",
    "utils.save_df_to_csv(syn_loss_history, output_loss_history, indexing=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
